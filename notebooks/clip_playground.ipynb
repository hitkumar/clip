{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get rid of this dirty hack\n",
    "import os\n",
    "os.getcwd()\n",
    "import sys\n",
    "sys.path.append(\"/Users/htkumar/clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:20<00:00, 17.2MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clip.model.CLIP"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"../CLIP.png\")).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x16566a0e0>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_features = model.encode_image(image)\n",
    "print(image_features.shape)\n",
    "text_features = model.encode_text(text)\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (100. * (image_features @ text_features.T)).softmax(dim=-1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[25.5528, 20.0899, 19.7495]], grad_fn=<MmBackward0>),\n",
       " tensor([[25.5528],\n",
       "         [20.0899],\n",
       "         [19.7495]], grad_fn=<TBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.tokenize([\"A deer is here\", \"A beer is here\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /Users/htkumar/.cache/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:02<00:00, 60161875.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/htkumar/.cache/cifar-100-python.tar.gz to /Users/htkumar/.cache\n"
     ]
    }
   ],
   "source": [
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32>, 49)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar100[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "image_input = preprocess(image).unsqueeze(0)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 224, 224]), torch.Size([100, 77]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_input.shape, text_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([100, 512]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape, text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100. * image_features @ text_features.T).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1976e-03, 5.4627e-03, 4.8136e-03, 1.8082e-04, 1.8419e-04, 3.4513e-04,\n",
       "         5.7727e-04, 8.2103e-03, 9.9322e-05, 3.7705e-04, 1.0402e-03, 8.2095e-04,\n",
       "         3.4284e-05, 1.0731e-04, 1.9522e-03, 1.4169e-04, 1.7230e-03, 4.6303e-05,\n",
       "         4.2151e-03, 9.5372e-04, 2.7544e-04, 1.2663e-03, 3.1629e-04, 5.2210e-05,\n",
       "         6.0814e-04, 1.6915e-04, 7.3051e-03, 1.7485e-02, 5.9205e-04, 4.7084e-03,\n",
       "         1.2483e-05, 4.3250e-04, 8.4593e-03, 1.0473e-04, 1.4937e-05, 5.7394e-04,\n",
       "         9.1998e-04, 1.1684e-04, 1.0331e-04, 2.5124e-04, 1.1973e-03, 6.8170e-03,\n",
       "         1.7164e-02, 8.7136e-04, 1.8750e-02, 1.4284e-04, 3.7086e-04, 1.1403e-04,\n",
       "         1.0234e-04, 4.6626e-04, 6.6153e-03, 2.9241e-03, 6.3651e-04, 7.6152e-04,\n",
       "         1.2286e-04, 1.4347e-04, 8.5427e-04, 1.4071e-04, 2.5623e-04, 1.5453e-03,\n",
       "         8.2367e-04, 1.7338e-03, 5.6944e-04, 3.3849e-04, 4.9928e-04, 6.9673e-04,\n",
       "         4.8177e-04, 6.3734e-04, 7.4546e-06, 1.2440e-05, 1.7304e-04, 7.2544e-05,\n",
       "         1.1135e-03, 5.2110e-05, 1.3243e-03, 2.5226e-04, 2.8802e-05, 1.5945e-02,\n",
       "         6.5313e-01, 2.8920e-03, 1.0557e-04, 6.0129e-05, 7.8724e-04, 3.8252e-02,\n",
       "         2.0946e-04, 6.4963e-03, 1.5895e-03, 2.6247e-04, 2.3472e-03, 1.1592e-04,\n",
       "         1.8628e-04, 4.3682e-04, 1.5310e-04, 1.2288e-01, 7.3358e-05, 1.7735e-05,\n",
       "         1.0798e-04, 7.8451e-05, 3.5900e-04, 8.4517e-03]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1976e-03, 5.4627e-03, 4.8136e-03, 1.8082e-04, 1.8419e-04, 3.4513e-04,\n",
       "        5.7727e-04, 8.2103e-03, 9.9322e-05, 3.7705e-04, 1.0402e-03, 8.2095e-04,\n",
       "        3.4284e-05, 1.0731e-04, 1.9522e-03, 1.4169e-04, 1.7230e-03, 4.6303e-05,\n",
       "        4.2151e-03, 9.5372e-04, 2.7544e-04, 1.2663e-03, 3.1629e-04, 5.2210e-05,\n",
       "        6.0814e-04, 1.6915e-04, 7.3051e-03, 1.7485e-02, 5.9205e-04, 4.7084e-03,\n",
       "        1.2483e-05, 4.3250e-04, 8.4593e-03, 1.0473e-04, 1.4937e-05, 5.7394e-04,\n",
       "        9.1998e-04, 1.1684e-04, 1.0331e-04, 2.5124e-04, 1.1973e-03, 6.8170e-03,\n",
       "        1.7164e-02, 8.7136e-04, 1.8750e-02, 1.4284e-04, 3.7086e-04, 1.1403e-04,\n",
       "        1.0234e-04, 4.6626e-04, 6.6153e-03, 2.9241e-03, 6.3651e-04, 7.6152e-04,\n",
       "        1.2286e-04, 1.4347e-04, 8.5427e-04, 1.4071e-04, 2.5623e-04, 1.5453e-03,\n",
       "        8.2367e-04, 1.7338e-03, 5.6944e-04, 3.3849e-04, 4.9928e-04, 6.9673e-04,\n",
       "        4.8177e-04, 6.3734e-04, 7.4546e-06, 1.2440e-05, 1.7304e-04, 7.2544e-05,\n",
       "        1.1135e-03, 5.2110e-05, 1.3243e-03, 2.5226e-04, 2.8802e-05, 1.5945e-02,\n",
       "        6.5313e-01, 2.8920e-03, 1.0557e-04, 6.0129e-05, 7.8724e-04, 3.8252e-02,\n",
       "        2.0946e-04, 6.4963e-03, 1.5895e-03, 2.6247e-04, 2.3472e-03, 1.1592e-04,\n",
       "        1.8628e-04, 4.3682e-04, 1.5310e-04, 1.2288e-01, 7.3358e-05, 1.7735e-05,\n",
       "        1.0798e-04, 7.8451e-05, 3.5900e-04, 8.4517e-03])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)\n",
      "\n",
      "See :func:`torch.topk`\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "similarity[0].topk??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = similarity[0].topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           snake: 65.31%\n",
      "          turtle: 12.29%\n",
      "    sweet_pepper: 3.83%\n",
      "          lizard: 1.87%\n",
      "       crocodile: 1.75%\n"
     ]
    }
   ],
   "source": [
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100.*value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root = os.path.expanduser(\"~/.cache\")\n",
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True, train=False, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images)\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return torch.cat(all_features).numpy(), torch.cat(all_labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [10:01<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:06<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "test_features, test_labels = get_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
